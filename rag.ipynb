{"cells":[{"cell_type":"markdown","source":["# 1. Read File and Text Processing"],"metadata":{"id":"jD_dZB5OCq7-"}},{"cell_type":"markdown","metadata":{"id":"xjxVpsik2pyy"},"source":["## Import Dependencies"]},{"cell_type":"code","source":["# Google Colab installs\n","import os\n","\n","if \"COLAB_GPU\" in os.environ:\n","    !pip install -U torch\n","    !pip install --upgrade --force-reinstall PyMuPDF # for reading PDFs with Python\n","    !pip install sentence-transformers # for embedding models\n","    !pip install accelerate # for quantization model loading\n","    !pip install bitsandbytes # for quantizing models (less storage space)\n","    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference\n","    !pip install tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxVQwmKD22hI","executionInfo":{"status":"ok","timestamp":1732111877944,"user_tz":0,"elapsed":52056,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}},"outputId":"008fd701-df60-48ec-f4f9-481330545fe1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Collecting PyMuPDF\n","  Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n","Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyMuPDF\n","Successfully installed PyMuPDF-1.24.14\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.2)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.44.1\n","Collecting flash-attn\n","  Downloading flash_attn-2.7.0.post2.tar.gz (2.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n","Building wheels for collected packages: flash-attn\n","  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flash-attn: filename=flash_attn-2.7.0.post2-cp310-cp310-linux_x86_64.whl size=183291101 sha256=16a849d51b95cf8e47a6e6cd36826e9ffbbc068a8546e7e3501a598bd70905a6\n","  Stored in directory: /root/.cache/pip/wheels/bf/e3/ed/5e845387d52f2debd1bafb847bf3d774d3f0a3c8e31b1dc948\n","Successfully built flash-attn\n","Installing collected packages: flash-attn\n","Successfully installed flash-attn-2.7.0.post2\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"XUKrXdx22py1","executionInfo":{"status":"ok","timestamp":1732112080351,"user_tz":0,"elapsed":2,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}}},"outputs":[],"source":["import pandas as pd\n","import fitz\n","from spacy.lang.en import English\n","from sentence_transformers import SentenceTransformer\n","import requests"]},{"cell_type":"markdown","metadata":{"id":"H7Xx9jKs2py2"},"source":["## Read the PDF File"]},{"cell_type":"code","source":["pdf_path = 'RAG for LLM.pdf'\n","url = 'https://arxiv.org/pdf/2312.10997'\n","\n","if not os.path.exists(pdf_path):\n","    response = requests.get(url)\n","\n","    # Check if the request was successful\n","    if response.status_code == 200:\n","        # Open a file in binary write mode and save the content to it\n","        with open(pdf_path, \"wb\") as file:\n","            file.write(response.content)\n","        print(f\"The file has been downloaded and saved as {pdf_path}\")\n","    else:\n","        print(f\"Failed to download the file. Status code: {response.status_code}\")\n","else:\n","    print(f\"File {pdf_path} exists.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gCtB4OXO693r","executionInfo":{"status":"ok","timestamp":1732112095423,"user_tz":0,"elapsed":321,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}},"outputId":"840fce46-531b-4155-a86b-814df14846cf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["The file has been downloaded and saved as RAG for LLM.pdf\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNczlovZ2py2","executionInfo":{"status":"ok","timestamp":1732112100318,"user_tz":0,"elapsed":569,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}},"outputId":"a2105102-5039-4d7e-dcd7-bfdce4d5370a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["21"]},"metadata":{},"execution_count":9}],"source":["doc = fitz.open(pdf_path)\n","\n","pdf_file = []\n","for page_number, page in enumerate(doc):  # iterate the document pages\n","    text = page.get_text()  # get plain text encoded as UTF-8\n","    text = text.replace(\"\\n\", \" \").strip()\n","    pdf_file.append({\"page_number\": page_number,\n","                            \"page_char_count\": len(text),\n","                            \"page_word_count\": len(text.split(\" \")),\n","                            \"page_sentence_count_raw\": len(text.split(\". \")),\n","                            \"page_token_count\": len(text) / 4,\n","                            \"text\": text})\n","len(pdf_file)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"4UZnxqmp2py3","executionInfo":{"status":"ok","timestamp":1732112108143,"user_tz":0,"elapsed":607,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}},"outputId":"dbc4ce26-8fe1-46b9-cbf7-f829cf3d05b2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n","0            0             5451              739                       28   \n","1            1             3082              450                       32   \n","2            2             3950              575                       41   \n","3            3             3574              492                       30   \n","4            4             6362              887                       42   \n","\n","   page_token_count                                               text  \n","0           1362.75  1 Retrieval-Augmented Generation for Large Lan...  \n","1            770.50  2 Fig. 1. Technology tree of RAG research. The...  \n","2            987.50  3 Fig. 2. A representative instance of the RAG...  \n","3            893.50  4 Fig. 3. Comparison between the three paradig...  \n","4           1590.50  5 aligns the text more closely with data distr...  "],"text/html":["\n","  <div id=\"df-7bc650b0-5001-4f10-936c-2bf7d334164c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page_number</th>\n","      <th>page_char_count</th>\n","      <th>page_word_count</th>\n","      <th>page_sentence_count_raw</th>\n","      <th>page_token_count</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>5451</td>\n","      <td>739</td>\n","      <td>28</td>\n","      <td>1362.75</td>\n","      <td>1 Retrieval-Augmented Generation for Large Lan...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3082</td>\n","      <td>450</td>\n","      <td>32</td>\n","      <td>770.50</td>\n","      <td>2 Fig. 1. Technology tree of RAG research. The...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>3950</td>\n","      <td>575</td>\n","      <td>41</td>\n","      <td>987.50</td>\n","      <td>3 Fig. 2. A representative instance of the RAG...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3574</td>\n","      <td>492</td>\n","      <td>30</td>\n","      <td>893.50</td>\n","      <td>4 Fig. 3. Comparison between the three paradig...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>6362</td>\n","      <td>887</td>\n","      <td>42</td>\n","      <td>1590.50</td>\n","      <td>5 aligns the text more closely with data distr...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bc650b0-5001-4f10-936c-2bf7d334164c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7bc650b0-5001-4f10-936c-2bf7d334164c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7bc650b0-5001-4f10-936c-2bf7d334164c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f35fd77a-a044-498a-9893-2496921470e3\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f35fd77a-a044-498a-9893-2496921470e3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f35fd77a-a044-498a-9893-2496921470e3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 21,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 20,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0,\n          17,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2304,\n        \"min\": 517,\n        \"max\": 9073,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          5451,\n          8997,\n          3641\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 337,\n        \"min\": 77,\n        \"max\": 1302,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          739,\n          1302,\n          496\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 121,\n        \"min\": 1,\n        \"max\": 338,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          28,\n          32,\n          46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 576.2083842528963,\n        \"min\": 129.25,\n        \"max\": 2268.25,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          1362.75,\n          2249.25,\n          910.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wang a,c aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University cCollege of Design and Innovation, Tongji University Abstract\\u2014Large Language Models (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain- specific information. RAG synergistically merges LLMs\\u2019 intrin- sic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evalua- tion framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development 1. Index Terms\\u2014Large language model, retrieval-augmented gen- eration, natural language processing, information retrieval I. INTRODUCTION L ARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing \\u201challucinations\\u201d [2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applica- tions. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown Corresponding Author.Email:haofen.wang@tongji.edu.cn 1Resources are available at https://github.com/Tongji-KGLLM/ RAG-Survey in Figure 1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG\\u2019s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques [3]\\u2013[5].The subsequent arrival of ChatGPT [6] marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more com- plex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques. The burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of \\u201cRetrieval,\\u201d \\u201cGeneration,\\u201d and \\u201cAugmentation.\\u201d On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: \\u2022 In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, arXiv:2312.10997v5  [cs.CL]  27 Mar 2024\",\n          \"18 [44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong, O. Kuchaiev, B. Li, C. Xiao et al., \\u201cShall we pretrain autoregressive language models with retrieval? a comprehensive study,\\u201d arXiv preprint arXiv:2304.06762, 2023. [45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan- zaro, \\u201cInstructretro: Instruction tuning post retrieval-augmented pre- training,\\u201d arXiv preprint arXiv:2310.07713, 2023. [46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana, and S. Nanayakkara, \\u201cImproving the domain adaptation of retrieval augmented generation (rag) models for open domain question answer- ing,\\u201d Transactions of the Association for Computational Linguistics, vol. 11, pp. 1\\u201317, 2023. [47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, \\u201cAugmentation-adapted retriever improves generalization of language models as generic plug-in,\\u201d arXiv preprint arXiv:2305.17331, 2023. [48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, \\u201cMaking retrieval- augmented language models robust to irrelevant context,\\u201d arXiv preprint arXiv:2310.01558, 2023. [49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, \\u201cUnderstanding re- trieval augmentation for long-form question answering,\\u201d arXiv preprint arXiv:2310.12150, 2023. [50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, \\u201cChain-of-note: Enhancing robustness in retrieval-augmented language models,\\u201d arXiv preprint arXiv:2311.09210, 2023. [51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, \\u201cSearch-in-the- chain: Towards accurate, credible and traceable large language models for knowledgeintensive tasks,\\u201d CoRR, vol. abs/2304.14732, 2023. [52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat, \\u201cOptimizing retrieval-augmented reader models via token elimination,\\u201d arXiv preprint arXiv:2310.13682, 2023. [53] J. L\\u00b4ala, O. O\\u2019Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques, and A. D. White, \\u201cPaperqa: Retrieval-augmented generative agent for scientific research,\\u201d arXiv preprint arXiv:2312.07559, 2023. [54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano, Y. Maarek, N. Tonellotto, and F. Silvestri, \\u201cThe power of noise: Redefining retrieval for rag systems,\\u201d arXiv preprint arXiv:2401.14887, 2024. [55] Z. Zhang, X. Zhang, Y. Ren, S. Shi, M. Han, Y. Wu, R. Lai, and Z. Cao, \\u201cIag: Induction-augmented generation framework for answer- ing reasoning questions,\\u201d in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 1\\u201314. [56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo, D. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al., \\u201cNomiracl: Knowing when you don\\u2019t know for robust multilingual retrieval-augmented generation,\\u201d arXiv preprint arXiv:2312.11361, 2023. [57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, \\u201cTree of clarifica- tions: Answering ambiguous questions with retrieval-augmented large language models,\\u201d arXiv preprint arXiv:2310.14696, 2023. [58] Y. Wang, P. Li, M. Sun, and Y. Liu, \\u201cSelf-knowledge guided retrieval augmentation for large language models,\\u201d arXiv preprint arXiv:2310.05002, 2023. [59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, \\u201cRetrieval- generation synergy augmented large language models,\\u201d arXiv preprint arXiv:2310.05149, 2023. [60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, and B. Catanzaro, \\u201cRetrieval meets long context large language models,\\u201d arXiv preprint arXiv:2310.03025, 2023. [61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \\u201cInterleav- ing retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions,\\u201d arXiv preprint arXiv:2212.10509, 2022. [62] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.- R. Wen, and H. Wang, \\u201cInvestigating the factual knowledge boundary of large language models with retrieval augmentation,\\u201d arXiv preprint arXiv:2307.11019, 2023. [63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D. Manning, \\u201cRaptor: Recursive abstractive processing for tree-organized retrieval,\\u201d arXiv preprint arXiv:2401.18059, 2024. [64] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton- Brown, and Y. Shoham, \\u201cIn-context retrieval-augmented language models,\\u201d arXiv preprint arXiv:2302.00083, 2023. [65] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, \\u201cRetrieve-and- sample: Document-level event argument extraction via hybrid retrieval augmentation,\\u201d in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 293\\u2013306. [66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, \\u201cZemi: Learning zero-shot semi-parametric language models from multiple tasks,\\u201d arXiv preprint arXiv:2210.00185, 2022. [67] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, \\u201cCorrective retrieval augmented generation,\\u201d arXiv preprint arXiv:2401.15884, 2024. [68] P. Jain, L. B. Soares, and T. Kwiatkowski, \\u201c1-pager: One pass answer generation and evidence retrieval,\\u201d arXiv preprint arXiv:2310.16568, 2023. [69] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, \\u201cPrca: Fitting black-box large language models for retrieval question answer- ing via pluggable reward-driven contextual adapter,\\u201d arXiv preprint arXiv:2310.18347, 2023. [70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, \\u201cOpen-source large language models are strong zero-shot query likelihood models for document ranking,\\u201d arXiv preprint arXiv:2310.13243, 2023. [71] F. Xu, W. Shi, and E. Choi, \\u201cRecomp: Improving retrieval-augmented lms with compression and selective augmentation,\\u201d arXiv preprint arXiv:2310.04408, 2023. [72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle- moyer, and W.-t. Yih, \\u201cReplug: Retrieval-augmented black-box lan- guage models,\\u201d arXiv preprint arXiv:2301.12652, 2023. [73] E. Melz, \\u201cEnhancing llm intelligence with arm-rag: Auxiliary ra- tionale memory for retrieval augmented generation,\\u201d arXiv preprint arXiv:2311.04177, 2023. [74] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi, J. Z. Pan, and K.-F. Wong, \\u201cUnims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems,\\u201d arXiv preprint arXiv:2401.13256, 2024. [75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang, \\u201cAugmented large language models with parametric knowledge guid- ing,\\u201d arXiv preprint arXiv:2305.04757, 2023. [76] X. Li, Z. Liu, C. Xiong, S. Yu, Y. Gu, Z. Liu, and G. Yu, \\u201cStructure- aware language model pretraining improves dense retrieval on struc- tured data,\\u201d arXiv preprint arXiv:2305.19912, 2023. [77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, \\u201cKnowledge graph-augmented language models for knowledge-grounded dialogue generation,\\u201d arXiv preprint arXiv:2305.18846, 2023. [78] W. Shen, Y. Gao, C. Huang, F. Wan, X. Quan, and W. Bi, \\u201cRetrieval- generation alignment for end-to-end task-oriented dialogue system,\\u201d arXiv preprint arXiv:2310.08877, 2023. [79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, \\u201cDual-feedback knowledge retrieval for task-oriented dialogue systems,\\u201d arXiv preprint arXiv:2310.14528, 2023. [80] P. Ranade and A. Joshi, \\u201cFabula: Intelligence report generation using retrieval-augmented narrative construction,\\u201d arXiv preprint arXiv:2310.13848, 2023. [81] X. Jiang, R. Zhang, Y. Xu, R. Qiu, Y. Fang, Z. Wang, J. Tang, H. Ding, X. Chu, J. Zhao et al., \\u201cThink and retrieval: A hypothesis knowledge graph enhanced medical large language models,\\u201d arXiv preprint arXiv:2312.15883, 2023. [82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang, \\u201cKnowledge-augmented language model verification,\\u201d arXiv preprint arXiv:2310.12836, 2023. [83] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, \\u201cReasoning on graphs: Faithful and interpretable large language model reasoning,\\u201d arXiv preprint arXiv:2310.01061, 2023. [84] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun, X. Bresson, and B. Hooi, \\u201cG-retriever: Retrieval-augmented generation for textual graph understanding and question answering,\\u201d arXiv preprint arXiv:2402.07630, 2024. [85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su, X. Li, A. Su et al., \\u201cTablegpt: Towards unifying tables, nature language and commands into one gpt,\\u201d arXiv preprint arXiv:2307.08674, 2023. [86] M. Gaur, K. Gunaratna, V. Srinivasan, and H. Jin, \\u201cIseeq: Information seeking question generation using dynamic meta-information retrieval and knowledge graphs,\\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672\\u201310 680. [87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch\\u00a8arli, and D. Zhou, \\u201cLarge language models can be easily distracted by irrelevant context,\\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 31 210\\u201331 227. [88] R. Teja, \\u201cEvaluating the ideal chunk size for a rag system using llamaindex,\\u201d https://www.llamaindex.ai/blog/ evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5, 2023.\",\n          \"16 Fig. 6. Summary of RAG ecosystem initial learning curve. 3) Specialization - optimizing RAG to better serve production environments. The mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational technology stack, laying the groundwork for advanced enterprise applications. However, a fully integrated, comprehensive platform concept is still in the future, requiring further innovation and development. F. Multi-modal RAG RAG has transcended its initial text-based question- answering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains: Image. RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2 [177] leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zero- shot image-to-text conversions. The \\u201cVisualize Before You Write\\u201d method [178] employs image generation to steer the LM\\u2019s text generation, showing promise in open-ended text generation tasks. Audio and Video. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data [179]. UEOP marks a significant ad- vancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text con- version [180]. Additionally, KNN-based attention fusion lever- ages audio embeddings and semantically related text embed- dings to refine ASR, thereby accelerating domain adaptation. Vid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence [181]. Code. RBPS [182] excels in small-scale learning tasks by retrieving code examples that align with developers\\u2019 objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion genera- tion and program repair. For structured knowledge, the CoK method [106] first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks. VIII. CONCLUSION The summary of this paper, as depicted in Figure 6, empha- sizes RAG\\u2019s significant advancement in enhancing the capa- bilities of LLMs by integrating parameterized knowledge from language models with extensive non-parameterized data from external knowledge bases. The survey showcases the evolution of RAG technologies and their application on many different tasks. The analysis outlines three developmental paradigms within the RAG framework: Naive, Advanced, and Modu- lar RAG, each representing a progressive enhancement over its predecessors. RAG\\u2019s technical integration with other AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG\\u2019s application scope is expanding into multimodal do- mains, adapting its principles to interpret and process diverse data forms like images, videos, and code. This expansion high- lights RAG\\u2019s significant practical implications for AI deploy- ment, attracting interest from academic and industrial sectors.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":10}],"source":["df = pd.DataFrame(pdf_file)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"mab8K8Y02py3"},"source":["## Text Processing"]},{"cell_type":"markdown","metadata":{"id":"R7A1iJP62py4"},"source":["### Sentencizer"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uu3_Vpvx2py4","executionInfo":{"status":"ok","timestamp":1732112175779,"user_tz":0,"elapsed":554,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}},"outputId":"fa624adb-1b35-4750-99c7-f5310847e60b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[First sentences., Second sentence.]"]},"metadata":{},"execution_count":12}],"source":["nlp = English()\n","\n","# Add a sentencizer pipeline\n","nlp.add_pipe(\"sentencizer\")\n","# Test\n","list(nlp(\"First sentences. Second sentence.\").sents)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"kWQF5HEP2py4","executionInfo":{"status":"ok","timestamp":1732112177514,"user_tz":0,"elapsed":529,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}},"outputId":"0c61872b-7814-41bd-822b-5cf2b1b87a22"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n","0            0             5451              739                       28   \n","1            1             3082              450                       32   \n","2            2             3950              575                       41   \n","3            3             3574              492                       30   \n","4            4             6362              887                       42   \n","\n","   page_token_count                                               text  \\\n","0           1362.75  1 Retrieval-Augmented Generation for Large Lan...   \n","1            770.50  2 Fig. 1. Technology tree of RAG research. The...   \n","2            987.50  3 Fig. 2. A representative instance of the RAG...   \n","3            893.50  4 Fig. 3. Comparison between the three paradig...   \n","4           1590.50  5 aligns the text more closely with data distr...   \n","\n","                                      sentences_list  \n","0  [(1, Retrieval, -, Augmented, Generation, for,...  \n","1  [(2, Fig, .), (1, .), (Technology, tree, of, R...  \n","2  [(3, Fig, .), (2, .), (A, representative, inst...  \n","3  [(4, Fig, .), (3, .), (Comparison, between, th...  \n","4  [(5, aligns, the, text, more, closely, with, d...  "],"text/html":["\n","  <div id=\"df-e9b3eec7-67fc-47b1-8ec8-0cb6cdfb002c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page_number</th>\n","      <th>page_char_count</th>\n","      <th>page_word_count</th>\n","      <th>page_sentence_count_raw</th>\n","      <th>page_token_count</th>\n","      <th>text</th>\n","      <th>sentences_list</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>5451</td>\n","      <td>739</td>\n","      <td>28</td>\n","      <td>1362.75</td>\n","      <td>1 Retrieval-Augmented Generation for Large Lan...</td>\n","      <td>[(1, Retrieval, -, Augmented, Generation, for,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3082</td>\n","      <td>450</td>\n","      <td>32</td>\n","      <td>770.50</td>\n","      <td>2 Fig. 1. Technology tree of RAG research. The...</td>\n","      <td>[(2, Fig, .), (1, .), (Technology, tree, of, R...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>3950</td>\n","      <td>575</td>\n","      <td>41</td>\n","      <td>987.50</td>\n","      <td>3 Fig. 2. A representative instance of the RAG...</td>\n","      <td>[(3, Fig, .), (2, .), (A, representative, inst...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3574</td>\n","      <td>492</td>\n","      <td>30</td>\n","      <td>893.50</td>\n","      <td>4 Fig. 3. Comparison between the three paradig...</td>\n","      <td>[(4, Fig, .), (3, .), (Comparison, between, th...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>6362</td>\n","      <td>887</td>\n","      <td>42</td>\n","      <td>1590.50</td>\n","      <td>5 aligns the text more closely with data distr...</td>\n","      <td>[(5, aligns, the, text, more, closely, with, d...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9b3eec7-67fc-47b1-8ec8-0cb6cdfb002c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e9b3eec7-67fc-47b1-8ec8-0cb6cdfb002c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e9b3eec7-67fc-47b1-8ec8-0cb6cdfb002c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-9ec31687-1419-4665-81cd-069331b34904\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ec31687-1419-4665-81cd-069331b34904')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-9ec31687-1419-4665-81cd-069331b34904 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 21,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 20,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0,\n          17,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2304,\n        \"min\": 517,\n        \"max\": 9073,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          5451,\n          8997,\n          3641\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 337,\n        \"min\": 77,\n        \"max\": 1302,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          739,\n          1302,\n          496\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 121,\n        \"min\": 1,\n        \"max\": 338,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          28,\n          32,\n          46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 576.2083842528963,\n        \"min\": 129.25,\n        \"max\": 2268.25,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          1362.75,\n          2249.25,\n          910.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"1 Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng Wangc, and Haofen Wang a,c aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University cCollege of Design and Innovation, Tongji University Abstract\\u2014Large Language Models (LLMs) showcase impres- sive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain- specific information. RAG synergistically merges LLMs\\u2019 intrin- sic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evalua- tion framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development 1. Index Terms\\u2014Large language model, retrieval-augmented gen- eration, natural language processing, information retrieval I. INTRODUCTION L ARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing \\u201challucinations\\u201d [2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applica- tions. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown Corresponding Author.Email:haofen.wang@tongji.edu.cn 1Resources are available at https://github.com/Tongji-KGLLM/ RAG-Survey in Figure 1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG\\u2019s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques [3]\\u2013[5].The subsequent arrival of ChatGPT [6] marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more com- plex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques. The burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of \\u201cRetrieval,\\u201d \\u201cGeneration,\\u201d and \\u201cAugmentation.\\u201d On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: \\u2022 In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, arXiv:2312.10997v5  [cs.CL]  27 Mar 2024\",\n          \"18 [44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong, O. Kuchaiev, B. Li, C. Xiao et al., \\u201cShall we pretrain autoregressive language models with retrieval? a comprehensive study,\\u201d arXiv preprint arXiv:2304.06762, 2023. [45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan- zaro, \\u201cInstructretro: Instruction tuning post retrieval-augmented pre- training,\\u201d arXiv preprint arXiv:2310.07713, 2023. [46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana, and S. Nanayakkara, \\u201cImproving the domain adaptation of retrieval augmented generation (rag) models for open domain question answer- ing,\\u201d Transactions of the Association for Computational Linguistics, vol. 11, pp. 1\\u201317, 2023. [47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, \\u201cAugmentation-adapted retriever improves generalization of language models as generic plug-in,\\u201d arXiv preprint arXiv:2305.17331, 2023. [48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, \\u201cMaking retrieval- augmented language models robust to irrelevant context,\\u201d arXiv preprint arXiv:2310.01558, 2023. [49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, \\u201cUnderstanding re- trieval augmentation for long-form question answering,\\u201d arXiv preprint arXiv:2310.12150, 2023. [50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, \\u201cChain-of-note: Enhancing robustness in retrieval-augmented language models,\\u201d arXiv preprint arXiv:2311.09210, 2023. [51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, \\u201cSearch-in-the- chain: Towards accurate, credible and traceable large language models for knowledgeintensive tasks,\\u201d CoRR, vol. abs/2304.14732, 2023. [52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat, \\u201cOptimizing retrieval-augmented reader models via token elimination,\\u201d arXiv preprint arXiv:2310.13682, 2023. [53] J. L\\u00b4ala, O. O\\u2019Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques, and A. D. White, \\u201cPaperqa: Retrieval-augmented generative agent for scientific research,\\u201d arXiv preprint arXiv:2312.07559, 2023. [54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano, Y. Maarek, N. Tonellotto, and F. Silvestri, \\u201cThe power of noise: Redefining retrieval for rag systems,\\u201d arXiv preprint arXiv:2401.14887, 2024. [55] Z. Zhang, X. Zhang, Y. Ren, S. Shi, M. Han, Y. Wu, R. Lai, and Z. Cao, \\u201cIag: Induction-augmented generation framework for answer- ing reasoning questions,\\u201d in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 1\\u201314. [56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo, D. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al., \\u201cNomiracl: Knowing when you don\\u2019t know for robust multilingual retrieval-augmented generation,\\u201d arXiv preprint arXiv:2312.11361, 2023. [57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, \\u201cTree of clarifica- tions: Answering ambiguous questions with retrieval-augmented large language models,\\u201d arXiv preprint arXiv:2310.14696, 2023. [58] Y. Wang, P. Li, M. Sun, and Y. Liu, \\u201cSelf-knowledge guided retrieval augmentation for large language models,\\u201d arXiv preprint arXiv:2310.05002, 2023. [59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, \\u201cRetrieval- generation synergy augmented large language models,\\u201d arXiv preprint arXiv:2310.05149, 2023. [60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, and B. Catanzaro, \\u201cRetrieval meets long context large language models,\\u201d arXiv preprint arXiv:2310.03025, 2023. [61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \\u201cInterleav- ing retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions,\\u201d arXiv preprint arXiv:2212.10509, 2022. [62] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.- R. Wen, and H. Wang, \\u201cInvestigating the factual knowledge boundary of large language models with retrieval augmentation,\\u201d arXiv preprint arXiv:2307.11019, 2023. [63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D. Manning, \\u201cRaptor: Recursive abstractive processing for tree-organized retrieval,\\u201d arXiv preprint arXiv:2401.18059, 2024. [64] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton- Brown, and Y. Shoham, \\u201cIn-context retrieval-augmented language models,\\u201d arXiv preprint arXiv:2302.00083, 2023. [65] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, \\u201cRetrieve-and- sample: Document-level event argument extraction via hybrid retrieval augmentation,\\u201d in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 293\\u2013306. [66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, \\u201cZemi: Learning zero-shot semi-parametric language models from multiple tasks,\\u201d arXiv preprint arXiv:2210.00185, 2022. [67] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, \\u201cCorrective retrieval augmented generation,\\u201d arXiv preprint arXiv:2401.15884, 2024. [68] P. Jain, L. B. Soares, and T. Kwiatkowski, \\u201c1-pager: One pass answer generation and evidence retrieval,\\u201d arXiv preprint arXiv:2310.16568, 2023. [69] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, \\u201cPrca: Fitting black-box large language models for retrieval question answer- ing via pluggable reward-driven contextual adapter,\\u201d arXiv preprint arXiv:2310.18347, 2023. [70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, \\u201cOpen-source large language models are strong zero-shot query likelihood models for document ranking,\\u201d arXiv preprint arXiv:2310.13243, 2023. [71] F. Xu, W. Shi, and E. Choi, \\u201cRecomp: Improving retrieval-augmented lms with compression and selective augmentation,\\u201d arXiv preprint arXiv:2310.04408, 2023. [72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle- moyer, and W.-t. Yih, \\u201cReplug: Retrieval-augmented black-box lan- guage models,\\u201d arXiv preprint arXiv:2301.12652, 2023. [73] E. Melz, \\u201cEnhancing llm intelligence with arm-rag: Auxiliary ra- tionale memory for retrieval augmented generation,\\u201d arXiv preprint arXiv:2311.04177, 2023. [74] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi, J. Z. Pan, and K.-F. Wong, \\u201cUnims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems,\\u201d arXiv preprint arXiv:2401.13256, 2024. [75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang, \\u201cAugmented large language models with parametric knowledge guid- ing,\\u201d arXiv preprint arXiv:2305.04757, 2023. [76] X. Li, Z. Liu, C. Xiong, S. Yu, Y. Gu, Z. Liu, and G. Yu, \\u201cStructure- aware language model pretraining improves dense retrieval on struc- tured data,\\u201d arXiv preprint arXiv:2305.19912, 2023. [77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, \\u201cKnowledge graph-augmented language models for knowledge-grounded dialogue generation,\\u201d arXiv preprint arXiv:2305.18846, 2023. [78] W. Shen, Y. Gao, C. Huang, F. Wan, X. Quan, and W. Bi, \\u201cRetrieval- generation alignment for end-to-end task-oriented dialogue system,\\u201d arXiv preprint arXiv:2310.08877, 2023. [79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, \\u201cDual-feedback knowledge retrieval for task-oriented dialogue systems,\\u201d arXiv preprint arXiv:2310.14528, 2023. [80] P. Ranade and A. Joshi, \\u201cFabula: Intelligence report generation using retrieval-augmented narrative construction,\\u201d arXiv preprint arXiv:2310.13848, 2023. [81] X. Jiang, R. Zhang, Y. Xu, R. Qiu, Y. Fang, Z. Wang, J. Tang, H. Ding, X. Chu, J. Zhao et al., \\u201cThink and retrieval: A hypothesis knowledge graph enhanced medical large language models,\\u201d arXiv preprint arXiv:2312.15883, 2023. [82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang, \\u201cKnowledge-augmented language model verification,\\u201d arXiv preprint arXiv:2310.12836, 2023. [83] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, \\u201cReasoning on graphs: Faithful and interpretable large language model reasoning,\\u201d arXiv preprint arXiv:2310.01061, 2023. [84] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun, X. Bresson, and B. Hooi, \\u201cG-retriever: Retrieval-augmented generation for textual graph understanding and question answering,\\u201d arXiv preprint arXiv:2402.07630, 2024. [85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su, X. Li, A. Su et al., \\u201cTablegpt: Towards unifying tables, nature language and commands into one gpt,\\u201d arXiv preprint arXiv:2307.08674, 2023. [86] M. Gaur, K. Gunaratna, V. Srinivasan, and H. Jin, \\u201cIseeq: Information seeking question generation using dynamic meta-information retrieval and knowledge graphs,\\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672\\u201310 680. [87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch\\u00a8arli, and D. Zhou, \\u201cLarge language models can be easily distracted by irrelevant context,\\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 31 210\\u201331 227. [88] R. Teja, \\u201cEvaluating the ideal chunk size for a rag system using llamaindex,\\u201d https://www.llamaindex.ai/blog/ evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5, 2023.\",\n          \"16 Fig. 6. Summary of RAG ecosystem initial learning curve. 3) Specialization - optimizing RAG to better serve production environments. The mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational technology stack, laying the groundwork for advanced enterprise applications. However, a fully integrated, comprehensive platform concept is still in the future, requiring further innovation and development. F. Multi-modal RAG RAG has transcended its initial text-based question- answering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains: Image. RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2 [177] leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zero- shot image-to-text conversions. The \\u201cVisualize Before You Write\\u201d method [178] employs image generation to steer the LM\\u2019s text generation, showing promise in open-ended text generation tasks. Audio and Video. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data [179]. UEOP marks a significant ad- vancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text con- version [180]. Additionally, KNN-based attention fusion lever- ages audio embeddings and semantically related text embed- dings to refine ASR, thereby accelerating domain adaptation. Vid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence [181]. Code. RBPS [182] excels in small-scale learning tasks by retrieving code examples that align with developers\\u2019 objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion genera- tion and program repair. For structured knowledge, the CoK method [106] first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks. VIII. CONCLUSION The summary of this paper, as depicted in Figure 6, empha- sizes RAG\\u2019s significant advancement in enhancing the capa- bilities of LLMs by integrating parameterized knowledge from language models with extensive non-parameterized data from external knowledge bases. The survey showcases the evolution of RAG technologies and their application on many different tasks. The analysis outlines three developmental paradigms within the RAG framework: Naive, Advanced, and Modu- lar RAG, each representing a progressive enhancement over its predecessors. RAG\\u2019s technical integration with other AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG\\u2019s application scope is expanding into multimodal do- mains, adapting its principles to interpret and process diverse data forms like images, videos, and code. This expansion high- lights RAG\\u2019s significant practical implications for AI deploy- ment, attracting interest from academic and industrial sectors.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":13}],"source":["df['sentences_list'] = df['text'].apply(lambda text: list(nlp(text).sents))\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"BbQ3pc062py4"},"source":["### Check Token Limitation"]},{"cell_type":"markdown","metadata":{"id":"iLb-gt8C2py4"},"source":["Since I plan to use `all-mpnet-base-v2` model which has a capacity of 384 tokens to embed the text.\n","\n","So, the text has to be splitted into chunks to make sure that they are not exceed the model's capacity."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xzwOGidy2py5","executionInfo":{"status":"ok","timestamp":1732112181412,"user_tz":0,"elapsed":331,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}},"outputId":"f6268695-25cc-4299-a471-2af463fbdd80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Page 0 has 227.5 tokens\n","Page 1 has 136.5 tokens\n","Page 2 has 168.0 tokens\n","Page 3 has 151.75 tokens\n","Page 4 has 274.25 tokens\n","Page 5 has 211.0 tokens\n","Page 6 has 185.5 tokens\n","Page 7 has 272.0 tokens\n","Page 8 has 283.0 tokens\n","Page 9 has 269.25 tokens\n","Page 10 has 179.5 tokens\n","Page 11 has 261.5 tokens\n","Page 12 has 231.75 tokens\n","Page 13 has 189.25 tokens\n","Page 14 has 157.5 tokens\n","Page 15 has 154.25 tokens\n","Page 16 has 502.25 tokens\n","Page 17 has 515.25 tokens\n","Page 18 has 501.5 tokens\n","Page 19 has 511.5 tokens\n","Page 20 has 30.75 tokens\n"]}],"source":["# Check the token count in each page\n","for page in range(len(df['sentences_list'])):\n","    token = 0\n","    for text in df['sentences_list'][page]:\n","        token += len(text)/4\n","    print(f\"Page {page} has {token} tokens\")\n"]},{"cell_type":"markdown","metadata":{"id":"kN_vm16p2py5"},"source":["As we can see that the only pages that exceed 384 tokens are from Page 16 onward.\n","\n","However, those pages are about the references which don't have much important information, so I decided to drop them."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"VSeUoY3m2py5","executionInfo":{"status":"ok","timestamp":1732112478291,"user_tz":0,"elapsed":551,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}}},"outputs":[],"source":["df.drop([16, 17, 18, 19, 20], inplace=True)"]},{"cell_type":"markdown","source":["### Split each sentence into new DataFrame"],"metadata":{"id":"BZTzYZDtA8R3"}},{"cell_type":"code","source":["splitted_sentences = []\n","for page in range(len(df)):\n","    for sentence in df.iloc[page]['sentences_list']:\n","        sentences_dict = {}\n","        sentences_dict['page'] = page\n","        sentences_dict['sentences'] = sentence\n","        sentences_dict['token_count'] = len(sentence) / 4\n","        splitted_sentences.append(sentences_dict)\n","\n","sentences_df = pd.DataFrame(splitted_sentences)\n","sentences_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"dQE22j_9-ZZB","executionInfo":{"status":"ok","timestamp":1732113554560,"user_tz":0,"elapsed":607,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}},"outputId":"717acb59-53de-4c6a-a702-52533e4fb33f"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     page                                          sentences  token_count\n","0       0  (1, Retrieval, -, Augmented, Generation, for, ...        26.75\n","1       0  (Retrieval, -, Augmented, Generation, (, RAG, ...         5.00\n","2       0  (This, enhances, the, accuracy, and, credibili...         7.50\n","3       0  (RAG, synergistically, merges, LLMs, ’, intrin...         4.50\n","4       0  (This, comprehensive, review, paper, offers, a...         7.25\n","..    ...                                                ...          ...\n","494    15  (The, analysis, outlines, three, developmental...         7.25\n","495    15  (RAG, ’s, technical, integration, with, other,...         6.00\n","496    15  (Despite, the, progress, in, RAG, technology, ...         5.75\n","497    15  (RAG, ’s, application, scope, is, expanding, i...         7.25\n","498    15  (This, expansion, high-, lights, RAG, ’s, sign...         5.50\n","\n","[499 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-7bdc72cc-3103-4733-bb27-08c87a637751\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page</th>\n","      <th>sentences</th>\n","      <th>token_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>(1, Retrieval, -, Augmented, Generation, for, ...</td>\n","      <td>26.75</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>(Retrieval, -, Augmented, Generation, (, RAG, ...</td>\n","      <td>5.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>(This, enhances, the, accuracy, and, credibili...</td>\n","      <td>7.50</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>(RAG, synergistically, merges, LLMs, ’, intrin...</td>\n","      <td>4.50</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>(This, comprehensive, review, paper, offers, a...</td>\n","      <td>7.25</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>494</th>\n","      <td>15</td>\n","      <td>(The, analysis, outlines, three, developmental...</td>\n","      <td>7.25</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>15</td>\n","      <td>(RAG, ’s, technical, integration, with, other,...</td>\n","      <td>6.00</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>15</td>\n","      <td>(Despite, the, progress, in, RAG, technology, ...</td>\n","      <td>5.75</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>15</td>\n","      <td>(RAG, ’s, application, scope, is, expanding, i...</td>\n","      <td>7.25</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>15</td>\n","      <td>(This, expansion, high-, lights, RAG, ’s, sign...</td>\n","      <td>5.50</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>499 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bdc72cc-3103-4733-bb27-08c87a637751')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7bdc72cc-3103-4733-bb27-08c87a637751 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7bdc72cc-3103-4733-bb27-08c87a637751');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-9fdb1f4d-c147-45fb-ba1f-194ba6652dcb\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9fdb1f4d-c147-45fb-ba1f-194ba6652dcb')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-9fdb1f4d-c147-45fb-ba1f-194ba6652dcb button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_87d0c54d-3cd6-4d3f-a554-8e9ec34c2b7b\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('sentences_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_87d0c54d-3cd6-4d3f-a554-8e9ec34c2b7b button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('sentences_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"sentences_df","summary":"{\n  \"name\": \"sentences_df\",\n  \"rows\": 499,\n  \"fields\": [\n    {\n      \"column\": \"page\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 499,\n        \"samples\": [\n          \"The development of the RAG ecosystem is greatly impacted by the progression of its technical stack.\",\n          \"To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks.\",\n          \"Data sum- maries are stored at each node, aiding in the swift traversal of data and assisting the RAG system in determining which chunks to extract.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.047811030209827,\n        \"min\": 0.25,\n        \"max\": 231.75,\n        \"num_unique_values\": 62,\n        \"samples\": [\n          17.25,\n          18.0,\n          26.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["## Embedding"],"metadata":{"id":"czsnrp687675"}},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"ylBkaSMc2py5","executionInfo":{"status":"ok","timestamp":1732114127752,"user_tz":0,"elapsed":9659,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}},"outputId":"e3eb34a6-408c-443e-9225-8ec7d1e0e759"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   page                                          sentences  token_count  \\\n","0     0  (1, Retrieval, -, Augmented, Generation, for, ...        26.75   \n","1     0  (Retrieval, -, Augmented, Generation, (, RAG, ...         5.00   \n","2     0  (This, enhances, the, accuracy, and, credibili...         7.50   \n","3     0  (RAG, synergistically, merges, LLMs, ’, intrin...         4.50   \n","4     0  (This, comprehensive, review, paper, offers, a...         7.25   \n","\n","                                           embedding  \n","0  [0.033680752, 0.06904238, -0.03743913, 0.05732...  \n","1  [0.043078717, 0.05558917, -0.021187901, -0.018...  \n","2  [-0.011808507, -0.03139684, -0.040654678, -0.0...  \n","3  [-0.059846584, 0.028356424, -0.031254996, 0.01...  \n","4  [0.009409682, 0.003933026, 0.006201101, -0.064...  "],"text/html":["\n","  <div id=\"df-c308704a-2782-4d46-94c1-bae79e7f8102\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page</th>\n","      <th>sentences</th>\n","      <th>token_count</th>\n","      <th>embedding</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>(1, Retrieval, -, Augmented, Generation, for, ...</td>\n","      <td>26.75</td>\n","      <td>[0.033680752, 0.06904238, -0.03743913, 0.05732...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>(Retrieval, -, Augmented, Generation, (, RAG, ...</td>\n","      <td>5.00</td>\n","      <td>[0.043078717, 0.05558917, -0.021187901, -0.018...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>(This, enhances, the, accuracy, and, credibili...</td>\n","      <td>7.50</td>\n","      <td>[-0.011808507, -0.03139684, -0.040654678, -0.0...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>(RAG, synergistically, merges, LLMs, ’, intrin...</td>\n","      <td>4.50</td>\n","      <td>[-0.059846584, 0.028356424, -0.031254996, 0.01...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>(This, comprehensive, review, paper, offers, a...</td>\n","      <td>7.25</td>\n","      <td>[0.009409682, 0.003933026, 0.006201101, -0.064...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c308704a-2782-4d46-94c1-bae79e7f8102')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c308704a-2782-4d46-94c1-bae79e7f8102 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c308704a-2782-4d46-94c1-bae79e7f8102');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-46e4be8b-9f58-4ffb-8a43-db858a6a48c8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-46e4be8b-9f58-4ffb-8a43-db858a6a48c8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-46e4be8b-9f58-4ffb-8a43-db858a6a48c8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"sentences_df","summary":"{\n  \"name\": \"sentences_df\",\n  \"rows\": 499,\n  \"fields\": [\n    {\n      \"column\": \"page\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 499,\n        \"samples\": [\n          \"The development of the RAG ecosystem is greatly impacted by the progression of its technical stack.\",\n          \"To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks.\",\n          \"Data sum- maries are stored at each node, aiding in the swift traversal of data and assisting the RAG system in determining which chunks to extract.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.047811030209827,\n        \"min\": 0.25,\n        \"max\": 231.75,\n        \"num_unique_values\": 62,\n        \"samples\": [\n          17.25,\n          18.0,\n          26.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":43}],"source":["embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n","                                      device=\"cuda\")\n","\n","\n","sentences_df['embedding'] = sentences_df['sentences'].apply(lambda sentences: embedding_model.encode(sentences.text))\n","sentences_df.head()"]},{"cell_type":"markdown","source":["# 2. RAG Implementation"],"metadata":{"id":"mn-HrqmmCxfc"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Convert embeddings to torch tensor\n","embeddings = torch.tensor(np.array(sentences_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n","embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lwMgoktU7zZb","executionInfo":{"status":"ok","timestamp":1732115448244,"user_tz":0,"elapsed":321,"user":{"displayName":"Waris Ratthapoom","userId":"07979000213568698383"}},"outputId":"fc0e74eb-c370-474d-d4be-ff1b4593f395"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([499, 768])"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":[],"metadata":{"id":"RJPxSM4oEHJT"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}